{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the file path is correct or provide an alternative path to a valid CSV file\n",
    "df = pd.read_csv('/ev_workspace/data/sap.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Display information about the dataframe\n",
    "print(df.info())\n",
    "\n",
    "# Display statistical summary of the dataframe\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Start'] = pd.to_datetime(df['Start'])\n",
    "df['End'] = pd.to_datetime(df['End'])\n",
    "\n",
    "df['power'] = df['Energy'] / df['Charge.Duration'] * 60\n",
    "df['charge_end'] = df['Start'] + pd.to_timedelta(df['Charge.Duration'], unit='min')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Start' and 'End' columns to datetime\n",
    "df['Start'] = pd.to_datetime(df['Start'])\n",
    "df['End'] = pd.to_datetime(df['End'])\n",
    "\n",
    "# Define the start and end times\n",
    "start_time = pd.Timestamp('2017-06-01 09:00')\n",
    "end_time = df['End'].max()\n",
    "\n",
    "# Create a date range with 15-minute intervals\n",
    "date_range = pd.date_range(start=start_time, end=end_time, freq='15min')\n",
    "\n",
    "# Create a new DataFrame with the date range\n",
    "new_df = pd.DataFrame(date_range, columns=['start'])\n",
    "new_df['end'] = new_df['start'] + pd.Timedelta(minutes=15)\n",
    "\n",
    "print(new_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new column in new_df to store the summed power values\n",
    "new_df['power_0'] = 0.0\n",
    "\n",
    "# Iterate over each row in new_df\n",
    "for i, row in new_df.iterrows():\n",
    "    # Define the current start and end times\n",
    "    current_start = row['start']\n",
    "    current_end = row['end']\n",
    "    \n",
    "    # Filter the rows in df where the current interval overlaps with Start and charge_end\n",
    "    mask = (df['Start'] < current_end) & (df['charge_end'] > current_start)\n",
    "    \n",
    "    # Sum the power values for the filtered rows\n",
    "    summed_power = df.loc[mask, 'power'].sum()\n",
    "    \n",
    "    # Assign the summed power value to the new column in new_df\n",
    "    new_df.at[i, 'power_0'] = summed_power\n",
    "\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    new_df[f'power_{i}'] = new_df['power_0'].shift(-i)\n",
    "\n",
    "print(new_df.head())\n",
    "\n",
    "print(new_df.shape)\n",
    "\n",
    "# new_df = new_df.iloc[:50000]\n",
    "\n",
    "print(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "X_time = pd.DataFrame()\n",
    "X_time['hour_sin'] = np.sin(2 * np.pi * pd.to_datetime(new_df['start']).dt.hour / 24)\n",
    "X_time['month_sin'] = np.sin(2 * np.pi * pd.to_datetime(new_df['start']).dt.month / 12)\n",
    "X_time['weekday_sin'] = np.sin(2 * np.pi * pd.to_datetime(new_df['start']).dt.weekday / 7)\n",
    "X_time['weekofyear_sin'] = np.sin(2 * np.pi * pd.to_datetime(new_df['start']).dt.isocalendar().week / 52)\n",
    "X_time['quarter_sin'] = np.sin(2 * np.pi * pd.to_datetime(new_df['start']).dt.quarter / 4)\n",
    "X_time['is_weekend'] = (pd.to_datetime(new_df['start']).dt.weekday >= 5).astype(int)\n",
    "\n",
    "X_time['lag_1'] = new_df['power_0'].shift(1)  # Viivästetty arvo 1 aika-askeleen päähän\n",
    "X_time['lag_2'] = new_df['power_0'].shift(2)  # Viivästetty arvo 2 aika-askeleen päähän\n",
    "X_time['lag_3'] = new_df['power_0'].shift(3)  # Viivästetty arvo 2 aika-askeleen päähän\n",
    "X_time['lag_4'] = new_df['power_0'].shift(4)  # Viivästetty arvo 2 aika-askeleen päähän\n",
    "X_time['lag_8'] = new_df['power_0'].shift(8)  # Viivästetty arvo 7 aika-askeleen päähän (esim. viikon takainen arvo)\n",
    "\n",
    "# 2. Liikkuvat keskiarvot (\"rolling averages\")\n",
    "X_time['rolling_mean_3'] = new_df['power_0'].rolling(window=3).mean()  # 3:n edellisen havaintojakson keskiarvo\n",
    "X_time['rolling_mean_7'] = new_df['power_0'].rolling(window=7).mean()  # 7:n edellisen havaintojakson keskiarvo\n",
    "\n",
    "# 3. Liikkuvat keskihajonnat (\"rolling standard deviations\")\n",
    "X_time['rolling_std_3'] = new_df['power_0'].rolling(window=3).std()  # 3:n edellisen havaintojakson keskihajonta\n",
    "X_time['rolling_std_7'] = new_df['power_0'].rolling(window=7).std()  # 7:n edellisen havaintojakson keskihajonta\n",
    "\n",
    "# 4. Kuukausi- ja viikkosummat\n",
    "X_time['rolling_sum_14'] = new_df['power_0'].rolling(window=14).sum()  # Kuukauden summat (olettaen päivittäiset tiedot)\n",
    "X_time['rolling_sum_7'] = new_df['power_0'].rolling(window=7).sum()    # Viikon summat\n",
    "X_time['rolling_sum_3'] = new_df['power_0'].rolling(window=3).sum()    # Viikon summat\n",
    "\n",
    "# 5. Edellisen päivän korkeimmat ja alhaisimmat arvot\n",
    "X_time['lag_max_3'] = new_df['power_0'].rolling(window=3).max()  # Edellisen päivän maksimiarvo\n",
    "X_time['lag_min_3'] = new_df['power_0'].rolling(window=3).min()  # Edellisen päivän minimiarvo\n",
    "X_time['lag_max_8'] = new_df['power_0'].rolling(window=8).max()  # Edellisen päivän maksimiarvo\n",
    "X_time['lag_min_8'] = new_df['power_0'].rolling(window=8).min()  # Edellisen päivän minimiarvo\n",
    "\n",
    "# Liikkuvat mediaanit (\"rolling medians\")\n",
    "X_time['rolling_median_3'] = new_df['power_0'].rolling(window=3).median()  # 3:n edellisen havaintojakson mediaani\n",
    "X_time['rolling_median_7'] = new_df['power_0'].rolling(window=7).median()  # 7:n edellisen havaintojakson mediaani\n",
    "X_time['rolling_median_14'] = new_df['power_0'].rolling(window=14).median()  # 30:n edellisen havaintojakson mediaani\n",
    "\n",
    "X_time = X_time.dropna()\n",
    "X_time = X_time.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Määritellään suhteet koulutus-, validointi- ja testidatalle\n",
    "training_ratio = 0.7\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Määritellään jakopisteet\n",
    "training_size = int(len(new_df) * training_ratio)\n",
    "validation_size = int(len(new_df) * validation_ratio)\n",
    "test_size = len(new_df) - training_size - validation_size\n",
    "# Jaetaan data kolmeen osaan\n",
    "training_data = new_df[:training_size]\n",
    "validation_data = new_df[training_size:training_size + validation_size]\n",
    "test_data = new_df[training_size + validation_size:]\n",
    "\n",
    "# Jaetaan myös X_time kolmeen osaan\n",
    "training_X_time = X_time[:training_size]\n",
    "validation_X_time = X_time[training_size:training_size + validation_size]\n",
    "test_X_time = X_time[training_size + validation_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "power_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on 'power_0' of the training data\n",
    "power_scaler.fit(training_data['power_0'].values.reshape(-1, 1))\n",
    "\n",
    "# Scale 'power_0' and target columns in the training data\n",
    "training_data_scaled = training_data.copy()\n",
    "training_data_scaled['power_0'] = power_scaler.transform(training_data[['power_0']])\n",
    "\n",
    "for i in range(4):\n",
    "    training_data_scaled[f'power_{i}'] = power_scaler.transform(training_data[[f'power_{i}']])\n",
    "\n",
    "# Scale 'power_0' and target columns in the validation data\n",
    "validation_data_scaled = validation_data.copy()\n",
    "validation_data_scaled['power_0'] = power_scaler.transform(validation_data[['power_0']])\n",
    "\n",
    "for i in range(4):\n",
    "    validation_data_scaled[f'power_{i}'] = power_scaler.transform(validation_data[[f'power_{i}']])\n",
    "\n",
    "# Scale 'power_0' and target columns in the test data\n",
    "test_data_scaled = test_data.copy()\n",
    "test_data_scaled['power_0'] = power_scaler.transform(test_data[['power_0']])\n",
    "\n",
    "for i in range(4):\n",
    "    test_data_scaled[f'power_{i}'] = power_scaler.transform(test_data[[f'power_{i}']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on 'power_0' of the training data\n",
    "times_scaler.fit(training_X_time)\n",
    "\n",
    "# Scale 'power_0' and target columns in the training data\n",
    "training_X_time_scaled = training_X_time.copy()\n",
    "training_X_time_scaled = times_scaler.transform(training_X_time)\n",
    "\n",
    "# Scale 'power_0' and target columns in the validation data\n",
    "validation_X_time_scaled = validation_X_time.copy()\n",
    "validation_X_time_scaled = times_scaler.transform(validation_X_time)\n",
    "\n",
    "# Scale 'power_0' and target columns in the test data\n",
    "test_X_time_scaled = test_X_time.copy()\n",
    "test_X_time_scaled = times_scaler.transform(test_X_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_window = 4\n",
    "import optuna\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input, concatenate, MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def combined_window_dataset(data, X_time, past_window, future_window, batch_size):\n",
    "    data_len = len(data)\n",
    "    \n",
    "    def generator():\n",
    "        i = past_window\n",
    "        while True:\n",
    "            X_batch = []\n",
    "            X_time_batch = []\n",
    "            y_batch = []\n",
    "            for _ in range(batch_size):\n",
    "                if i + past_window + future_window >= data_len:\n",
    "                    i = past_window  # Palataan alkuun, kun data loppuu\n",
    "\n",
    "                # Otetaan menneet arvot ikkunana (X_batch)\n",
    "                X = data['power_0'].values[i:i + past_window]\n",
    "                X_batch.append(X)\n",
    "\n",
    "                # Otetaan ajalliset ominaisuudet (X_time_batch)\n",
    "                X_time_batch.append(X_time[i])\n",
    "\n",
    "                # Otetaan tulevaisuuden arvot, joita halutaan ennustaa (y_batch)\n",
    "                y = data['power_0'].values[i + past_window: i + past_window + future_window]\n",
    "                y_batch.append(y)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # Muunnetaan numpy-arrayksi ja muotoillaan se oikeaan muotoon\n",
    "            X_batch = np.array(X_batch).reshape(batch_size, past_window, 1)  # Muodoksi (batch_size, past_window, 1)\n",
    "            X_time_batch = np.array(X_time_batch).reshape(batch_size, -1)  # Muodoksi (batch_size, feature_count)\n",
    "            y_batch = np.array(y_batch).reshape(batch_size, future_window)  # Muodoksi (batch_size, future_window)\n",
    "\n",
    "            yield (X_batch, X_time_batch), y_batch\n",
    "\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(batch_size, past_window, 1), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(batch_size, X_time.shape[1]), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(batch_size, future_window), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
    "\n",
    "\n",
    "# Generaattori koulutusta varten\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optuna - CNN mallin optimointi\n",
    "\n",
    "def create_model(trial, past_window, future_window):\n",
    "    # Optuna hakee hyperparametrit\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)    \n",
    "    stride = trial.suggest_int(\"stride\", 2, 10)\n",
    "    n_dense_units_combined_1 = trial.suggest_int(\"n_dense_units_combined_1\", 32, 256, log=True)\n",
    "    dropout_rate_1 = trial.suggest_float(\"dropout_rate_1\", 0.1, 0.8)\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(past_window, 1))\n",
    "\n",
    "    # Ensimmäinen pää (kuten alkuperäinen malli)\n",
    "    n_filters_1 = trial.suggest_int(\"n_filters_1\", 4, 128, log=True)\n",
    "    kernel_size_1 = trial.suggest_int(\"kernel_size_1\", 2, 10)\n",
    "    pooling_1 = trial.suggest_categorical(\"pooling_1\", ['max', 'average', 'global_max', 'global_average'])\n",
    "    \n",
    "    x1 = Conv1D(filters=n_filters_1, kernel_size=kernel_size_1, activation='relu')(input_layer)\n",
    "    if pooling_1 == 'max':\n",
    "        x1 = tf.keras.layers.MaxPooling1D(pool_size=2)(x1)\n",
    "    elif pooling_1 == 'global_max':\n",
    "        x1 = GlobalMaxPooling1D()(x1)\n",
    "    elif pooling_1 == 'global_average':\n",
    "        x1 = GlobalAveragePooling1D()(x1)\n",
    "    else:\n",
    "        x1 = tf.keras.layers.AveragePooling1D(pool_size=2)(x1)\n",
    "\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    # Toinen pää (optimoitu stride)\n",
    "    n_filters_2 = trial.suggest_int(\"n_filters_2\", 4, 128, log=True)\n",
    "    kernel_size_2 = trial.suggest_int(\"kernel_size_2\", 2, 10)\n",
    "    pooling_2 = trial.suggest_categorical(\"pooling_2\", ['max', 'average', 'global_max', 'global_average'])\n",
    "    \n",
    "    x2 = Conv1D(filters=n_filters_2, kernel_size=kernel_size_2, strides=stride, activation='relu')(input_layer)\n",
    "    if pooling_2 == 'max':\n",
    "        x2 = tf.keras.layers.MaxPooling1D(pool_size=2)(x2)\n",
    "    elif pooling_2 == 'global_max':\n",
    "        x2 = GlobalMaxPooling1D()(x2)\n",
    "    elif pooling_2 == 'global_average':\n",
    "        x2 = GlobalAveragePooling1D()(x2)\n",
    "    else:\n",
    "        x2 = tf.keras.layers.AveragePooling1D(pool_size=2)(x2)\n",
    "        \n",
    "    x2 = Flatten()(x2)\n",
    "\n",
    "    # Kolmas pää (kaksi 1D CNN-kerrosta allekkain)\n",
    "    n_filters_3 = trial.suggest_int(\"n_filters_3\", 4, 128, log=True)   \n",
    "    kernel_size_3 = trial.suggest_int(\"kernel_size_3\", 2, 10)\n",
    "    pooling_3 = trial.suggest_categorical(\"pooling_3\", ['max', 'average', 'global_max', 'global_average'])\n",
    "    \n",
    "    x3 = Conv1D(filters=n_filters_3, kernel_size=kernel_size_3, activation='relu')(input_layer)    \n",
    "    if pooling_3 == 'max':\n",
    "        x3 = tf.keras.layers.MaxPooling1D(pool_size=2)(x3)\n",
    "    elif pooling_3 == 'global_max':\n",
    "        x3 = GlobalMaxPooling1D()(x3)\n",
    "    elif pooling_3 == 'global_average':\n",
    "        x3 = GlobalAveragePooling1D()(x3)\n",
    "    else:\n",
    "        x3 = tf.keras.layers.AveragePooling1D(pool_size=2)(x3)\n",
    "\n",
    "    x3 = Flatten()(x3)\n",
    "\n",
    "    input_time = Input(shape=(X_time.shape[1],), name=\"time_input\")\n",
    "\n",
    "    # Yhdistetään kaikki päät\n",
    "    combined = concatenate([x1, x2, x3, input_time])\n",
    "\n",
    "    # Syvä kerros yhdistetylle datalle\n",
    "    combined = Dense(n_dense_units_combined_1, activation='relu')(combined)\n",
    "    combined = Dropout(dropout_rate_1)(combined)\n",
    "    combined = Dense(n_dense_units_combined_1, activation='relu')(combined)\n",
    "    combined = Dropout(dropout_rate_1)(combined)\n",
    "\n",
    "    # Lopullinen ulostulo\n",
    "    output = Dense(future_window)(combined)\n",
    "\n",
    "    # Mallin luominen\n",
    "    model = Model(inputs=[input_layer, input_time], outputs=output)\n",
    "\n",
    "    # Optimizerin asetukset\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    try:\n",
    "        start_time = time.time()  # Start timing\n",
    "\n",
    "        # Hyperparameters\n",
    "        past_window = trial.suggest_int(\"past_window\", 100, 3000, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 32, 128, log=True)\n",
    "        future_window = 4  # As defined earlier\n",
    "\n",
    "        # Prepare datasets using scaled data\n",
    "        train_dataset = combined_window_dataset(\n",
    "            training_data_scaled, training_X_time_scaled, past_window, future_window, batch_size\n",
    "        ).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        validation_dataset = combined_window_dataset(\n",
    "            validation_data_scaled, validation_X_time_scaled, past_window, future_window, batch_size\n",
    "        ).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        # Model setup\n",
    "        model = create_model(trial, past_window=past_window, future_window=future_window)\n",
    "        steps_per_epoch_train = len(training_data_scaled) // batch_size\n",
    "        steps_per_epoch_val = len(validation_data_scaled) // batch_size\n",
    "\n",
    "        # Training parameters\n",
    "        EPOCHS = 20\n",
    "        val_losses = []\n",
    "        MAX_TIME = 300  # Maximum time per trial in seconds\n",
    "\n",
    "        # Training loop with pruning and time checking\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Check elapsed time\n",
    "            time_elapsed = time.time() - start_time\n",
    "            if time_elapsed > MAX_TIME:\n",
    "                print(f\"Trial {trial.number} stopped at epoch {epoch} due to time limit\")\n",
    "                raise optuna.TrialPruned()  # Prune if time limit exceeded\n",
    "\n",
    "            # Train for one epoch\n",
    "            history = model.fit(\n",
    "                train_dataset,\n",
    "                steps_per_epoch=steps_per_epoch_train,\n",
    "                validation_data=validation_dataset,\n",
    "                validation_steps=steps_per_epoch_val,\n",
    "                epochs=1,\n",
    "                verbose=2\n",
    "            )\n",
    "\n",
    "            # Append validation loss to list\n",
    "            val_losses.append(history.history['val_loss'][-1])\n",
    "\n",
    "            running_mean = np.mean(val_losses[-5:])                \n",
    "\n",
    "            print(f'Running mean of validation loss: {running_mean}')\n",
    "\n",
    "            trial.report(running_mean, step=epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                print(f\"Trial {trial.number} pruned at epoch {epoch}\")\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        running_mean = np.mean(val_losses[-5:])\n",
    "\n",
    "        # Clean up\n",
    "        del model, train_dataset, validation_dataset, history\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f'Time taken for this trial: {time.time() - start_time}')\n",
    "\n",
    "        return running_mean\n",
    "\n",
    "    except optuna.TrialPruned:\n",
    "        # Käsitellään prunatussa tilassa, ilman että merkitään virheeksi\n",
    "        print(f\"Trial {trial.number} pruned.\")\n",
    "        raise  # Pruned trials are re-raised to notify Optuna\n",
    "\n",
    "    except Exception as e:\n",
    "        # Käsitellään muut poikkeukset varsinaisina virheinä\n",
    "        print(f\"Trial {trial.number} failed with exception: {e}\")\n",
    "        trial.set_user_attr(\"failed\", True)\n",
    "        raise\n",
    "\n",
    "\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=10,      # Pruneri odottaa vähintään 10 kokeilua ennen kuin alkaa pruneamaan\n",
    "    n_warmup_steps=5,         # Alkuun vähintään 5 epochia, jotta mallin trendi saadaan näkyviin\n",
    "    interval_steps=1          # Prunaus tapahtuu joka toisen epochin jälkeen\n",
    ")\n",
    "\n",
    "sampler = TPESampler(\n",
    "    n_startup_trials=10,      # Aluksi käytetään satunnaisotantaa 10 kokeilun ajan\n",
    "    n_ei_candidates=24,       # Joka vaiheessa valitaan 24 ehdokasta Expected Improvement -estimaatin avulla\n",
    "    multivariate=True,        # Multivariaattinen versio TPE:stä, joka voi parantaa konvergenssia\n",
    "    group=True                # Asettaa ehdokkaat ryhmiksi, joka voi johtaa parempiin päätöksiin\n",
    ")\n",
    "\n",
    "\n",
    "# Optuna tutkimuksen suoritus SQL-tietokannan kanssa\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",  # Maksimoidaan kulmakerrointa\n",
    "    sampler=sampler,  # Käytetään määriteltyä TPE-sampleria\n",
    "    pruner=pruner,    # Käytetään määriteltyä median pruneria\n",
    "    storage=\"sqlite:///optuna_study.db\",  # SQLite-tietokanta\n",
    "    load_if_exists=True,\n",
    "    study_name='2024_10_29_slopet_tyo'  # Lataa olemassa oleva tutkimus, jos se on olemassa\n",
    ")\n",
    "\n",
    "study.optimize(objective, timeout= 0.5 * 3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model(best_params, past_window, future_window):\n",
    "    # Parametrien noutaminen suoraan sanakirjasta\n",
    "    learning_rate = best_params[\"learning_rate\"]    \n",
    "    stride = best_params[\"stride\"]\n",
    "    n_dense_units_combined_1 = best_params[\"n_dense_units_combined_1\"]\n",
    "    dropout_rate_1 = best_params[\"dropout_rate_1\"]\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(past_window, 1))\n",
    "\n",
    "    # Ensimmäinen pää (kuten alkuperäinen malli)\n",
    "    n_filters_1 = best_params[\"n_filters_1\"]\n",
    "    kernel_size_1 = best_params[\"kernel_size_1\"]\n",
    "    pooling_1 = best_params[\"pooling_1\"]\n",
    "\n",
    "    x1 = Conv1D(filters=n_filters_1, kernel_size=kernel_size_1, activation='relu')(input_layer)\n",
    "    if pooling_1 == 'max':\n",
    "        x1 = MaxPooling1D(pool_size=2)(x1)\n",
    "    else:\n",
    "        x1 = AveragePooling1D(pool_size=2)(x1)\n",
    "\n",
    "    x1 = Conv1D(filters=n_filters_1, kernel_size=kernel_size_1, activation='relu')(x1)\n",
    "    if pooling_1 == 'max':\n",
    "        x1 = MaxPooling1D(pool_size=2)(x1)\n",
    "    else:\n",
    "        x1 = AveragePooling1D(pool_size=2)(x1)\n",
    "\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    # Toinen pää (optimoitu stride)\n",
    "    n_filters_2 = best_params[\"n_filters_2\"]\n",
    "    kernel_size_2 = best_params[\"kernel_size_2\"]\n",
    "    pooling_2 = best_params[\"pooling_2\"]\n",
    "\n",
    "    x2 = Conv1D(filters=n_filters_2, kernel_size=kernel_size_2, strides=stride, activation='relu')(input_layer)\n",
    "    if pooling_2 == 'max':\n",
    "        x2 = MaxPooling1D(pool_size=2)(x2)\n",
    "    else:\n",
    "        x2 = AveragePooling1D(pool_size=2)(x2)\n",
    "\n",
    "    x2 = Conv1D(filters=n_filters_2, kernel_size=kernel_size_2, strides=stride, activation='relu')(x2)\n",
    "    if pooling_2 == 'max':\n",
    "        x2 = MaxPooling1D(pool_size=2)(x2)\n",
    "    else:\n",
    "        x2 = AveragePooling1D(pool_size=2)(x2)\n",
    "\n",
    "    x2 = Flatten()(x2)\n",
    "\n",
    "    # Kolmas pää (kaksi 1D CNN-kerrosta allekkain)\n",
    "    n_filters_3 = best_params[\"n_filters_3\"]\n",
    "    kernel_size_3 = best_params[\"kernel_size_3\"]\n",
    "    pooling_3 = best_params[\"pooling_3\"]\n",
    "\n",
    "    x3 = Conv1D(filters=n_filters_3, kernel_size=kernel_size_3, activation='relu')(input_layer)\n",
    "    if pooling_3 == 'max':\n",
    "        x3 = MaxPooling1D(pool_size=2)(x3)\n",
    "    else:\n",
    "        x3 = AveragePooling1D(pool_size=2)(x3)\n",
    "\n",
    "    x3 = Conv1D(filters=n_filters_3, kernel_size=kernel_size_3, activation='relu')(x3)\n",
    "    if pooling_3 == 'max':\n",
    "        x3 = MaxPooling1D(pool_size=2)(x3)\n",
    "    else:\n",
    "        x3 = AveragePooling1D(pool_size=2)(x3)\n",
    "\n",
    "    x3 = Flatten()(x3)\n",
    "\n",
    "    # Ajallinen syöte\n",
    "    input_time = Input(shape=(X_time.shape[1],), name=\"time_input\")\n",
    "\n",
    "    # Yhdistetään kaikki päät\n",
    "    combined = concatenate([x1, x2, x3, input_time])\n",
    "\n",
    "    # Syvä kerros yhdistetylle datalle\n",
    "    combined = Dense(n_dense_units_combined_1, activation='relu')(combined)\n",
    "    combined = Dropout(dropout_rate_1)(combined)\n",
    "    combined = Dense(n_dense_units_combined_1, activation='relu')(combined)\n",
    "    combined = Dropout(dropout_rate_1)(combined)\n",
    "\n",
    "    # Lopullinen ulostulo\n",
    "    output = Dense(future_window)(combined)\n",
    "\n",
    "    # Mallin luominen\n",
    "    model = Model(inputs=[input_layer, input_time], outputs=output)\n",
    "\n",
    "    # Optimizerin asetukset\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Ensure that 'power_scaler' is the same scaler used during training\n",
    "# If it was saved, load it; otherwise, make sure it's accessible here\n",
    "\n",
    "# Training the best model\n",
    "best_params = study.best_params\n",
    "batch_size = best_params.pop(\"batch_size\")\n",
    "past_window = best_params.pop(\"past_window\")\n",
    "future_window = 4  # As defined earlier\n",
    "\n",
    "# Create the final model with the best hyperparameters\n",
    "final_model = create_model(best_params, past_window=past_window, future_window=future_window)\n",
    "\n",
    "# Prepare datasets (ensure you're using scaled data)\n",
    "train_dataset = combined_window_dataset(training_data_scaled, training_X_time_scaled, past_window, future_window, batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = combined_window_dataset(validation_data_scaled, validation_X_time_scaled, past_window, future_window, batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "steps_per_epoch_train = len(training_data_scaled) // batch_size\n",
    "steps_per_epoch_val = len(validation_data_scaled) // batch_size\n",
    "\n",
    "\n",
    "final_model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_callback = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "final_model.fit(train_dataset,\n",
    "                validation_data=validation_dataset,\n",
    "                epochs=100,  # Adjust as needed\n",
    "                callbacks=[checkpoint_callback, reduce_lr_callback],\n",
    "                steps_per_epoch=steps_per_epoch_train,                \n",
    "                validation_steps=steps_per_epoch_val,                           \n",
    "                verbose=2)\n",
    "\n",
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Testing the model with test data\n",
    "y_true_scaled = []\n",
    "y_pred_scaled = []\n",
    "\n",
    "test_dataset = combined_window_dataset(test_data_scaled, test_X_time_scaled, past_window, future_window, batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Collect predictions and true values from the test set\n",
    "for (X_batch, X_time_batch), y_batch in test_dataset:\n",
    "    predictions = best_model.predict([X_batch, X_time_batch], verbose=0)\n",
    "    y_true_scaled.extend(y_batch.numpy())     # True values in scaled form\n",
    "    y_pred_scaled.extend(predictions)         # Predictions in scaled form\n",
    "    # Break after one full epoch to prevent infinite loop\n",
    "    if len(y_true_scaled) >= len(test_data_scaled) - past_window - future_window:\n",
    "        break\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_true_scaled = np.array(y_true_scaled)\n",
    "y_pred_scaled = np.array(y_pred_scaled)\n",
    "\n",
    "# Inverse-transform the scaled predictions and true values\n",
    "# Reshape if necessary to ensure correct dimensions\n",
    "y_true_inv = power_scaler.inverse_transform(y_true_scaled)\n",
    "y_pred_inv = power_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate MAE, MAPE, and MSE for each prediction window\n",
    "for i in range(future_window):\n",
    "\n",
    "    mae = mean_absolute_error(y_true_inv[:, i], y_pred_inv[:, i])\n",
    "    mape = mean_absolute_percentage_error(y_true_inv[:, i], y_pred_inv[:, i])\n",
    "    mse = mean_squared_error(y_true_inv[:, i], y_pred_inv[:, i])\n",
    "    print(f\"Prediction Step {i+1}:\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MAPE: {mape}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "# Overall metrics across all prediction steps\n",
    "overall_mae = mean_absolute_error(y_true_inv.flatten(), y_pred_inv.flatten())\n",
    "overall_mape = mean_absolute_percentage_error(y_true_inv.flatten(), y_pred_inv.flatten())\n",
    "overall_mse = mean_squared_error(y_true_inv.flatten(), y_pred_inv.flatten())\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"MAE: {overall_mae}\")\n",
    "print(f\"MAPE: {overall_mape}\")\n",
    "print(f\"MSE: {overall_mse}\")\n",
    "\n",
    "# Examine the first prediction\n",
    "print(\"First prediction (inverse-transformed):\", y_pred_inv[0])\n",
    "print(\"Actual values (inverse-transformed):\", y_true_inv[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
